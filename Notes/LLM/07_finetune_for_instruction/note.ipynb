{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Introduction to instruction fine-tuning\n",
    "\n",
    "- pretrained LLM involve training on text completion, and thus often struggle with instruction\n",
    "- need fine tune. preparing dataset is key\n",
    "\n",
    "<img src=\"pic1.jpg\" width=\"600\">\n",
    "\n",
    "- 3 stage of instruction fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Preparing a dataset for supervised instruction fine-tuning\n",
    "- use instructionâ€“response pairs to train LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "\n",
    "    # The book originally contained this unnecessary \"else\" clause:\n",
    "    #else:\n",
    "    #    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    #        text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- there are multiple ways to format the input output pairs\n",
    "    - known as `prompt styles`\n",
    "\n",
    "<img src=\"pic2.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert data into Alpaca-style\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    # the ### Input is skiped if input field is blank\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "print(model_input + desired_response)\n",
    "\n",
    "# without input as it is blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into train, test and validation set\n",
    "\n",
    "train_portion = int(len(data) * 0.85)  # 85% for training\n",
    "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Organizing data into training batches\n",
    "- pad the data sample to equal length so that can assemble multiple instruction example in batch\n",
    "- in previous:\n",
    "    - training batches created automatically by PyTorch `DataLoader`\n",
    "    - using default `collate` to combine list of sampes into batches\n",
    "- batching for instruction-based finetuning need to cerate custom collate function, to plug into `DataLoader`\n",
    "- step for batching\n",
    "\n",
    "<img src=\"pic3.jpg\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry) # step 2.1: format input\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append( # step 2.2: tokenize data\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- similar to classification fine-tuning, we want accelerate training by collecting multiple training example in batch.\n",
    "- so need to pad all input to similar length\n",
    "- similarly, pad with the token id of `<|endoftext|>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- move to step 2.3\n",
    "    - create a custom collate function to pass to data loader\n",
    "    - custom collate function pads training example in each batch to same length \n",
    "        - and allow different batch to have differetn length\n",
    "        - minimize unecessary padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2.3 adjust length with padding\n",
    "def custom_collate_draft_1(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    # and increase the max length by +1, which will add one extra\n",
    "    # padding token below\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to batch_max_length\n",
    "        padded = ( new_item + [pad_token_id] * (batch_max_length - len(new_item)))\n",
    "\n",
    "        # Via padded[:-1], we remove the extra padded token\n",
    "        # that has been added via the +1 setting in batch_max_length\n",
    "        # (the extra padding token will be relevant in later codes)\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "batch = (\n",
    " inputs_1,\n",
    " inputs_2,\n",
    " inputs_3\n",
    ")\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- after created custom collate function, need to create batches with target token ID correspond to input ID\n",
    "- the target token ID match to the input token ID, but with one position shifted to the right, + 1 padding token\n",
    "    - this allow LLM learn how to predict the next token in sequence.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "def custom_collate_draft_2(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- then assign a -100 placeholder value to all padding tokens\n",
    "    - this special value allow us to exclude padding tokens from contributing to training loss calculation\n",
    "    - when fine tuning for classification, only trained last ouput token, so no need to worry about this.\n",
    "- but we will retain 1 end-of-text token so that LLM learn when to generate end-of-text token to indicate response completed\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "\n",
    "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # New: Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "tensor(0.7936)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "    [-0.5, 1.5]]\n",
    ")\n",
    "targets_1 = torch.tensor([0, 1]) # Correct token indices to generate\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)\n",
    "\n",
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "    [-0.5, 1.5],\n",
    "    [-0.5, 1.5]]\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2) # adding additional token ID affect loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)\n",
    "\n",
    "# loss_3 and loss_1 is the same , cross entropy loss function ginored the third entry of -100!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in Pytroch, default setting for cross entropy function is  `cross_entropy(..., ignore_index=-100)`\n",
    "- also common to mask out instruction when calculating loss, so model trained to focus on generating accurate response rather than memorizing instruction\n",
    "\n",
    "<img src=\"pic4.jpg\" width=\"600\">\n",
    "\n",
    "- but some paper show that not masking instruction is actually  beneficial during instruction fine tuning\n",
    "\n",
    "## 7.4 Creating data loaders for an instruction dataset\n",
    "- after created `InstructionDataset` and `custom_collate_fn`, just need to plug them into PyTorch data loaders\n",
    "- the loaders will auto shuffle and organize batch for fine tuning process\n",
    "- `custom_collate_fn`with code to move input and target tensors to cpu/cuda/mps\n",
    "- previously, the data moved to target device in main training loop\n",
    "- if we move it as part in collate as background process, it wont block GPU when training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "# if torch.backends.mps.is_available():\n",
    "# device = torch.device(\"mps\")\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functools import partial\n",
    "\n",
    "# partials used to cerate a new function with some arg or kwargs prefilled\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024 # truncated by the max context length supported by GPT2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "num_workers = 0 # can increase worker number of parallel python process are supported by os\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)\n",
    "\n",
    "# 8 is batch size, 61 is number of token\n",
    "# first batch have 8*61 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,\n",
      "          985,   576,    13,   198,   198, 21017, 23412,    25,   198,   464,\n",
      "         5156,   318,   845, 13779,    13,   198,   198, 21017, 18261,    25,\n",
      "          198,   464,  5156,   318,   355, 13779,   355,   257,  4936,    13,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
      "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
      "        46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,   985,\n",
      "          576,    13,   198,   198, 21017, 23412,    25,   198,   464,  5156,\n",
      "          318,   845, 13779,    13,   198,   198, 21017, 18261,    25,   198,\n",
      "          464,  5156,   318,   355, 13779,   355,   257,  4936,    13, 50256,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../04_implementing_GPT_model\"))\n",
    "from simple_gpt import GPTModel,generate_text_simple\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../05_pretrain_on_unlabeled_data\"))\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from simple_pretrain import load_weights_into_gpt, text_to_token_ids, token_ids_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The baby is very cute.\n",
      "\n",
      "### Response:\n",
      "The baby is as cute as a button.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      " is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The baby is very cute.\n",
      "\n",
      "### Response:\n",
      "The baby is as cute as a button.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(token_ids_to_text(inputs[0],tokenizer))\n",
    "t=targets[0]\n",
    "print(token_ids_to_text(t[t>=0],tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Loading a pretrained LLM\n",
    "- need to load pretrain weight before fine tune\n",
    "- but this time we use 355 mil param\n",
    "    - 124 is too limited in capacity for instruction fine-tune\n",
    "    - smaller model lack the capacity to learn and retain pattern and behaviors requied for instruction related task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\355M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\355M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\355M\\vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download medium sized GPT model\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "# assess the pretrained LLM performance\n",
    "\n",
    "torch.manual_seed(123)\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../05_pretrain_on_unlabeled_data\"))\n",
    "from modify_output import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate(\n",
    " model=model,\n",
    " idx=text_to_token_ids(input_text, tokenizer),\n",
    " max_new_tokens=35,\n",
    " context_size=BASE_CONFIG[\"context_length\"],\n",
    " eos_id=50256,\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "# generated text contain both input and output text\n",
    "# when evaluating we want focus on model's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "response_text = generated_text[len(input_text):].strip() # subtract the length of input, leaving only the output\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it show that the pretrained model is not yet capable of following instruction\n",
    "- it just repeat only\n",
    "\n",
    "## 7.6 Fine-tuning the LLM on instruction data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse loss calculation and trainign function in ch05\n",
    "from simple_pretrain import calc_loss_loader, train_model_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.825896453857422\n",
      "Validation loss: 3.761921453475952\n"
     ]
    }
   ],
   "source": [
    "# calculate loss before training\n",
    "\n",
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(\n",
    "    train_loader, model, device, num_batches=5\n",
    "    )\n",
    "    val_loss = calc_loss_loader(\n",
    "        val_loader, model, device, num_batches=5\n",
    ")\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Ti\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())      # Should be True\n",
    "print(torch.cuda.get_device_name())\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626\n",
      "Ep 1 (Step 000005): Train loss 1.174, Val loss 1.102\n",
      "Ep 1 (Step 000010): Train loss 0.872, Val loss 0.945\n",
      "Ep 1 (Step 000015): Train loss 0.856, Val loss 0.906\n",
      "Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881\n",
      "Ep 1 (Step 000025): Train loss 0.753, Val loss 0.859\n",
      "Ep 1 (Step 000030): Train loss 0.798, Val loss 0.836\n",
      "Ep 1 (Step 000035): Train loss 0.714, Val loss 0.808\n",
      "Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806\n",
      "Ep 1 (Step 000045): Train loss 0.633, Val loss 0.790\n",
      "Ep 1 (Step 000050): Train loss 0.662, Val loss 0.783\n",
      "Ep 1 (Step 000055): Train loss 0.760, Val loss 0.764\n",
      "Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743\n",
      "Ep 1 (Step 000065): Train loss 0.652, Val loss 0.735\n",
      "Ep 1 (Step 000070): Train loss 0.532, Val loss 0.729\n",
      "Ep 1 (Step 000075): Train loss 0.569, Val loss 0.729\n",
      "Ep 1 (Step 000080): Train loss 0.605, Val loss 0.725\n",
      "Ep 1 (Step 000085): Train loss 0.509, Val loss 0.709\n",
      "Ep 1 (Step 000090): Train loss 0.562, Val loss 0.691\n",
      "Ep 1 (Step 000095): Train loss 0.500, Val loss 0.681\n",
      "Ep 1 (Step 000100): Train loss 0.502, Val loss 0.677\n",
      "Ep 1 (Step 000105): Train loss 0.564, Val loss 0.670\n",
      "Ep 1 (Step 000110): Train loss 0.555, Val loss 0.667\n",
      "Ep 1 (Step 000115): Train loss 0.508, Val loss 0.664\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\n",
      "Ep 2 (Step 000120): Train loss 0.435, Val loss 0.672\n",
      "Ep 2 (Step 000125): Train loss 0.450, Val loss 0.687\n",
      "Ep 2 (Step 000130): Train loss 0.447, Val loss 0.682\n",
      "Ep 2 (Step 000135): Train loss 0.405, Val loss 0.681\n",
      "Ep 2 (Step 000140): Train loss 0.409, Val loss 0.681\n",
      "Ep 2 (Step 000145): Train loss 0.368, Val loss 0.681\n",
      "Ep 2 (Step 000150): Train loss 0.381, Val loss 0.676\n",
      "Ep 2 (Step 000155): Train loss 0.412, Val loss 0.676\n",
      "Ep 2 (Step 000160): Train loss 0.415, Val loss 0.684\n",
      "Ep 2 (Step 000165): Train loss 0.379, Val loss 0.686\n",
      "Ep 2 (Step 000170): Train loss 0.323, Val loss 0.683\n",
      "Ep 2 (Step 000175): Train loss 0.337, Val loss 0.671\n",
      "Ep 2 (Step 000180): Train loss 0.392, Val loss 0.658\n",
      "Ep 2 (Step 000185): Train loss 0.415, Val loss 0.660\n",
      "Ep 2 (Step 000190): Train loss 0.340, Val loss 0.651\n",
      "Ep 2 (Step 000195): Train loss 0.330, Val loss 0.638\n",
      "Ep 2 (Step 000200): Train loss 0.310, Val loss 0.638\n",
      "Ep 2 (Step 000205): Train loss 0.351, Val loss 0.634\n",
      "Ep 2 (Step 000210): Train loss 0.366, Val loss 0.632\n",
      "Ep 2 (Step 000215): Train loss 0.396, Val loss 0.636\n",
      "Ep 2 (Step 000220): Train loss 0.300, Val loss 0.648\n",
      "Ep 2 (Step 000225): Train loss 0.348, Val loss 0.660\n",
      "Ep 2 (Step 000230): Train loss 0.294, Val loss 0.657\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom\n",
      "Training completed in 23.91 minutes.\n"
     ]
    }
   ],
   "source": [
    "# train the model!\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=0.00005, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- wow it take 24min to train on my CPU lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plot_losses import plot_losses\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- both validation and training are improved over the training\n",
    "\n",
    "## 7.7 Extracting and saving responses\n",
    "- to evaluate the performance:\n",
    "    1. extract model generated response for each input\n",
    "    2. evaluate the LLM to quntify the quality of response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    ")\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
